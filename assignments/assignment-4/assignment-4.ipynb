{"cells":[{"cell_type":"markdown","source":"## Assignment 4\n\n_Group 11: Alexandra Parkegren & Albin Sjöstrand_\n\n_Time Spent: yes, many moons has passed since we started_\n\n\nIn this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \nYou will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \n-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n-\thard-ham: non-spam messages more difficult to differentiate \n-\tspam: spam messages \n\nExecute the cell below to download and extract the data into the environment of the notebook.\nThe data will now be in the three folders `easy_ham`, `hard_ham`, and `spam`.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00000-e4cd9896-df3f-430e-a2df-98b452bd928c","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00000-084b92dc-b33d-4754-acc2-d0c663848a76","output_cleared":false,"source_hash":"318bae5f","execution_millis":0,"execution_start":1606913304950},"source":"#Download and extract data\n#!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n#!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n#!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n#!tar -xjf 20021010_easy_ham.tar.bz2\n#!tar -xjf 20021010_hard_ham.tar.bz2\n#!tar -xjf 20021010_spam.tar.bz2","execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 1. Preprocessing: \n1.\tNote that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text. Further down (in the higher-grade part), you will be asked to filter out the headers and footers. \n2.\tWe don’t want to train and test on the same data. Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`)\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00003-2a69b88f-56aa-4a30-8d17-58513522ab81","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-cc66e966-5cbd-4f6e-9060-1a6cfd232f86","output_cleared":false,"source_hash":"a3895e97","execution_millis":1,"execution_start":1606913304950},"source":"import pandas as pd\nimport tarfile\nfrom sklearn.model_selection import train_test_split","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00004-c559f0bb-baaa-429e-b6fb-bc37bb9e68ad","output_cleared":false,"source_hash":"1c61e3d0","execution_millis":1456,"execution_start":1606913304951},"source":"# Extract the email content, decode them, and convert as dataframe\ndef extract_files(files):  \n    rows = []\n    for fname in files:\n        # open the tar file\n        tfile = tarfile.open(fname, 'r:bz2')\n        for member in tfile.getmembers():\n            f = tfile.extractfile(member)\n            if f is not None:\n                row = f.read()\n                #get all the content of file as a row\n                rows.append({'message': row.decode('latin-1'), 'class': 'ham'})\n        tfile.close()\n    #return rows\n    return pd.DataFrame(rows)\n\n\n#get one dataframe with all of our files\ndf_ham = extract_files(['./20021010_easy_ham.tar.bz2','./20021010_hard_ham.tar.bz2'])\ndf_spam = extract_files(['./20021010_spam.tar.bz2'])\n\nhamtrain, hamtest = train_test_split(df_ham, test_size=0.25, random_state=0)\nspamtrain, spamtest = train_test_split(df_spam, test_size=0.25, random_state=0)","execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We used the tarfiles and unpacked them ourself and saved the emails as textfiles in a dataframe.\n\n Split the spam and the ham datasets in a training set and a test set. \n \nWhat does the task mean when the split should be named \"hamtrain, spamtrain, hamtest and spamtest\"? \nThe code above shows how we literally interpret this by splitting all ham-emails into train and test \nand then separately split the spam-emails into train and test. But we find this constellation confusion,\nIf we train only on spam and then test on spam, there will be 100% accuracy.\n\nInstead we join all emails into one big dataset and split that one instead to be able to train and predict. Hence, we interpret the task as:\n\"Split the combinet data (the spam and the ham datasets) in a combined training set and a test set\".\nLogically they could therefor be called x_train, x_test, y_train,y_test as in previous assignment but lets continue by calling them ham and \nspam train and test even though for example hamtest also inclused some spam. \nSee code below for this and notice how we label all data to differentiate between ham and spam.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00006-82da273a-d98c-414d-bddd-3d775e734114","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00006-52dd04f9-7200-4be4-ae26-c6b4f6a19a61","output_cleared":false,"source_hash":"2d38c234","execution_millis":1459,"execution_start":1606924091632},"source":"# Extract the email content, decode them, and convert as dataframe\ndef extract_mails(files,labels):\n\n    label = 0\n    rows  = []\n\n    # read and append for both ham files\n    for fname in files:\n        # open the tar file\n        hfile = tarfile.open(fname, 'r:bz2')\n        for member in hfile.getmembers():\n            f = hfile.extractfile(member)\n            if f is not None:\n                row = f.read()\n                #get all the content of file as a row\n                rows.append({'message': row.decode('latin-1'), 'class': labels[label]}) \n        hfile.close()\n        label +=1\n\n    # create a dataframe with message and class as rows\n    return pd.DataFrame(rows)\n\n# Extract the emails to a usable dataframe\nfiles = ['./20021010_easy_ham.tar.bz2','./20021010_hard_ham.tar.bz2','./20021010_spam.tar.bz2']\nlabels = ['ham','ham','spam']\ndf_mails = extract_mails(files,labels)\n\n# Divide the emails into train and test sets\nhamtrain, hamtest, spamtrain, spamtest = train_test_split(df_mails['message'], df_mails['class'], test_size=0.25, random_state=0)\n\nprint('Total mails:',df_mails.shape)\nprint('Ham train:  ',hamtrain.shape)\nprint('Ham test:   ',hamtest.shape)\nprint('Spam train: ',spamtrain.shape)\nprint('Spam test:  ',spamtest.shape)","execution_count":43,"outputs":[{"name":"stdout","text":"Total mails: (3302, 2)\nHam train:   (2476,)\nHam test:    (826,)\nSpam train:  (2476,)\nSpam test:   (826,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 2. Write a Python program that: \n1.\tUses four datasets (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \n2.\tTrains a Naïve Bayes classifier (e.g. Sklearn) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in SKlearn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem\n- Multinomial Naive Bayes  \n- Bernoulli Naive Bayes. \n\nPlease inspect the documentation to ensure input to the classifiers is appropriate. Discuss the differences between these two classifiers. \n\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00008-32a74643-726c-4cb4-8fea-e2ecab0fd83b","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00006-aa0a01b7-5037-496d-ab03-b1c9da254556","output_cleared":false,"source_hash":"aa7700f1","execution_millis":3849,"execution_start":1606924099521},"source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n\ndef calc_rates(spamtest,predictions):\n \n    #a normalized confusion matrix\n    tn, fp, fn, tp = confusion_matrix(spamtest, predictions, normalize='true').ravel()\n\n    return tp,fn\n\n\ndef NB(dataframe):\n    \n    #instantiate vectorizer\n    X = CountVectorizer().fit_transform(dataframe['message'])    \n    y = dataframe['class']\n\n    # Divide the emails into train and test sets\n    hamtrain, hamtest, spamtrain, spamtest = train_test_split(X, y, test_size=0.25, random_state = 0)\n\n    # instantiate a Multinomial Naive Bayes model\n    mnb_classifier = MultinomialNB()\n    mnb_classifier.fit(hamtrain, spamtrain)\n    mnb_predictions = mnb_classifier.predict(hamtest)\n\n    # instantiate a Bernoulli Naive Bayes model\n    bnb_classifier = BernoulliNB()\n    bnb_classifier.fit(hamtrain, spamtrain)\n    bnb_predictions = bnb_classifier.predict(hamtest)\n\n    # Calc TP and FN rates\n    mnb_tp, mnb_fn = calc_rates(spamtest,mnb_predictions)\n    bnb_tp, bnb_fn = calc_rates(spamtest,bnb_predictions)\n\n    return mnb_tp, mnb_fn, bnb_tp, bnb_fn\n\n\n# Extract the emails to a usable dataframe\nfiles = ['./20021010_easy_ham.tar.bz2','./20021010_hard_ham.tar.bz2','./20021010_spam.tar.bz2']\nlabels = ['ham','ham','spam']\ndf_mails = extract_mails(files,labels)\n\nmnb_tp, mnb_fn ,bnb_tp, bnb_fn = NB(df_mails)\n\nprint('Multinomial Naive Bayes model gives TP rate = %f and FN rate = %f' %(mnb_tp,mnb_fn))\nprint('Bernoulli   Naive Bayes model gives TP rate = %f and FN rate = %f' %(bnb_tp,bnb_fn))\n","execution_count":44,"outputs":[{"name":"stdout","text":"Multinomial Naive Bayes model gives TP rate = 0.862069 and FN rate = 0.137931\nBernoulli   Naive Bayes model gives TP rate = 0.248276 and FN rate = 0.751724\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Above we use extract_files to get a dataset with the mails we want.\nThen we use NB to first turn the data into vectors with CountVectorizer to then train the \ndata on two classifiers; Multinomial and Bernoulli Naive Bayes model.\nWe use our calculate_rates to get the true positive rate (predicted positive and is positive) \nand false negative rate (predicted negative but should have been postive). \n\nThe Naive Bayes models flip the matrix of TP, TN, FP, FN from the way Selpi presentet the matrix in class, we therefor use [sklearns](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\nconfusion_matrix to be sure which values correspond to TP and FN. \nThe task was to give the rates and we choose to calculate the rates by comparing the result \nto the whole set and a easy way to do this was to normalize the confusion matrix. \nTo normalize the confusion matrix we believe the result will always be more easy to understand.\n\nA good model that makes good prediction would give a high rate for TP and TN and low values for FP and FN.\nThe results show that the Multinomial Naive Bayes model  is much better at\npredicting positive results, in our case ham. MBM predicted less wrong on the negative/spam, \nnamely better at predicting the spam messages as well.\nIn the opposite way the Bernoulli Naive Bayes model did not do a good job at predicting positves \n(a low value of TP) or predicting negatives (a high value for FN).\n\n   ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00010-1c8d8cb2-fc60-4608-8b26-74dbd8ef30ee","output_cleared":false}},{"cell_type":"markdown","source":"### 3.Run your program on \n-\tSpam versus easy-ham \n-\tSpam versus hard-ham.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00007-63d9344f-5a9b-4e1f-a387-ddb700c61a4c","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00011-8c6e4f1c-3c7d-40be-b3c3-18b6d22cd0ac","output_cleared":false,"source_hash":"77830f5e","execution_millis":4113,"execution_start":1606913355478},"source":"df_easy_ham_spam = extract_mails(['./20021010_easy_ham.tar.bz2','./20021010_spam.tar.bz2'],['ham','spam'])\ndf_hard_ham_spam = extract_mails(['./20021010_hard_ham.tar.bz2','./20021010_spam.tar.bz2'],['ham','spam'])\n\ne_mnb_tp, e_mnb_fn, e_bnb_tp, e_bnb_fn = NB(df_easy_ham_spam)\nh_mnb_tp, h_mnb_fn, h_bnb_tp, h_bnb_fn = NB(df_hard_ham_spam)\n\nprint('Spam vs. easy-ham')\nprint('Multinomial Naive Bayes model gives TP rate = %f and FN rate = %f' %(e_mnb_tp,e_mnb_fn))\nprint('Bernoulli   Naive Bayes model gives TP rate = %f and FN rate = %f' %(e_bnb_tp,e_bnb_fn))\nprint()\nprint('Spam vs. hard-ham')\nprint('Multinomial Naive Bayes model gives TP rate = %f and FN rate = %f' %(h_mnb_tp,h_mnb_fn))\nprint('Bernoulli   Naive Bayes model gives TP rate = %f and FN rate = %f' %(h_bnb_tp,h_bnb_fn))\n","execution_count":15,"outputs":[{"name":"stdout","text":"Spam vs. easy-ham\nMultinomial Naive Bayes model gives TP rate = 0.862069 and FN rate = 0.137931\nBernoulli   Naive Bayes model gives TP rate = 0.517241 and FN rate = 0.482759\n\nSpam vs. hard-ham\nMultinomial Naive Bayes model gives TP rate = 0.961832 and FN rate = 0.038168\nBernoulli   Naive Bayes model gives TP rate = 0.969466 and FN rate = 0.030534\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\nThis way the model got less ham to train on, which mean a bigger percentage of the training data was spam.\nThis made the Bernouilli method performance better at predicting both ham (higher TP) and spam(lower FN). \nBut for the, already pretty good Multinomial method, there was no difference at all between _spam vs. easy-ham_ and _spam vs. easy- and hard-ham_.\n\nThe hard-ham messages are more similar to the spam compared to the easy-ham. \nWhich means, to differentiate the hard-ham message from the spam \nthe algoritm need to pick up more subtle differenses. Which in turn make the test more accurate.\nConsequently, when the models only got to train on the hard-spam all the \nresults got much better for both models.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00013-8e974733-2b73-422a-b6c6-4625ef2cba69"}},{"cell_type":"markdown","source":"### 4.\tTo avoid classification based on common and uninformative words it is common to filter these out. \n\n**a.** Argue why this may be useful. Try finding the words that are too common/uncommon in the dataset. \n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00011-bcb84d30-c1f6-4d4b-af4d-96179fd07243","output_cleared":false}},{"cell_type":"markdown","source":"See code down below were we find the most common and least common words in our data.\n\nTo filter out data is very common to optimize the performance further.\nIf we would start by filter out very common words, like \"I\", \"and\" and \"hello\". Then there will be less data to handle and it will be a faster process.\nIt would also be easier to train and the accuracy could get better since the model is being trained only on relevant words. Say for\nexample we take a common word such as \"you\", which is very likely to be part of a large part of the emails. It may not contribute to\nthe training of the model because it cannot be deterimined if it contribute to the classification being a ham or a spam.\nFinding relevant words will now run faster.\n\nBecause its a good thing to filter out data there already exists alot of help functions for this.\nStemming can be used to also filter out unformative words that does not contribute to the result. By trimming the words down to their stem\nwe can minimize the amount of words being used in the email, and as well help with the filtering of common words. For example the words\n\"learning\" would have a stem of \"learn\", and if we find that \"learn\" is a common word it would be included into the filtering out of them. \nThis can be further improved with Lemmitization, where Stemming is utilise but we also consider the context of that words, making it less likely\nto be trimmed down to its stem. With this being said, it's good to think of the performance since all these calculation would add to the amount\nof work that has to be done, especially on large datasets, where we want to optimize it.\n\nThe Natural Language Toolkit provide Tokenization, which removes words as well as exclamation point,commas, apostrophes, question marks commas etc.\nThis can be used to further filter out tokens that does not contribute to the email. TF-IDF is another method that counts how often a word appears and takes the lenght of the email into consideration.\nTF-IDF (Time frequency times inverse document frequency)\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00014-c9b5ba6b-9dde-4c22-922e-f2dc5cc87ff1","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00015-72a8956a-6b8c-4f6f-9be3-fdc4f90d324e","output_cleared":false,"source_hash":"138e6138","execution_millis":74900,"execution_start":1606919911412},"source":"from collections import Counter\nimport string\nimport itertools\n\n# get all words in our data as the type counter\ndef count_words():\n    # Extract the emails to a usable dataframe\n    df_mails = extract_mails(['./20021010_easy_ham.tar.bz2', './20021010_hard_ham.tar.bz2', './20021010_spam.tar.bz2'],['ham','ham','spam'])\n\n    #remove punctuation tokens with regex so at split \"Hello:\"\" will be splitted as \"Hello\"\n    df_mails['message'] = df_mails['message'].str.replace('[{}]'.format(string.punctuation), ' ')\n    df_mails['message'] = df_mails['message'].str.replace('\\n', ' ')\n    df_mails['message'] = df_mails['message'].str.replace('\\t', ' ')\n\n    # split the mails into words\n    mails_splitted = df_mails[\"message\"].str.split(\" \")\n\n    # count how many times a word occurs in all emails\n    word_counter = Counter()\n    for i in range(0,len(df_mails)):\n        word_counter = word_counter + Counter(mails_splitted[i])\n\n    return word_counter\n\n\n\nword_counter = count_words()\n\n#how many words most and least common we would like\nn_words = 10 \n\n#the least common words\nword_counter2 = word_counter\nleast_common_words = word_counter2.most_common()[:-n_words-1:-1]\nprint('The %d least common words are: %d' %(n_words, least_common_words))\n\n#the top common words\nmost_common_words = word_counter.most_common(n_words)\nprint('The %d most common words are:  %d ' %(n_words, most_common_words)) ","execution_count":41,"outputs":[{"name":"stdout","text":"Least common words:  [('7b1b73cf36cf9dbc3d64e3f2ee2b91f1', 1), ('00000', 1), ('cmds', 1), ('c4ff6dba0a5177d3c7d8ef54c8920496', 1), ('00099', 1), ('01d2958ccb7c2e4c02d0920593962436', 1), ('00098', 1), ('dce08392ba6bc552d13394fa73974b62', 1), ('00097', 1), ('b2cb600e893f7a663ea5f9bff3a6276e', 1)]\nMost common words:  [('', 2290248), ('com', 68985), ('0', 46616), ('the', 35612), ('1', 34693), ('http', 33960), ('a', 33238), ('2002', 28353), ('to', 25485), ('3D', 25396)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To be able to differentiate the words we started by removing some symbols because we noticed that \nthe emails contained alot of characters that were not words. \nSome characters also interrupt words so we want to replace them by a space to bea able to read words. \n\\n is an example of that because when the email was turned into a string a new line was translated to \\n. \n\nWe did keep some elements that we do not consider to be words, for example we can see a lot of them in the list of least common words.\nNeither did we remove single numbers. Becase we thought it would be too much of a manipulation.\n\nWe then splitted all emails into words so a Counter() could count how many times each word occur.\nIn the printed result we see the word inside the apostrophes and then how many times it occured in all of our emails.\nWe chose to print only the 10 most common and the 10 least common found words but we could easily change that by changing the parameter n_words.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00016-8263971e-ff13-4266-979f-76eae88b3bc1"}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00017-5b62d8f8-a293-4f82-a028-6ecb412dd9d5","output_cleared":false,"source_hash":"d1ddd05","execution_millis":76652,"execution_start":1606924347099},"source":"# get a array of strings with the most common words. how many depending on the input\ndef list_of_common_words(length_common_words):\n    \n    word_counter = count_words()\n\n    #get most common words in counter\n    most_common_words = word_counter.most_common(length_common_words)\n    \n\n    #get only the words\n    top_words = []\n    for i in most_common_words:\n        top_words.append(i[0])\n    \n    top_words.sort()\n    return top_words\n\n\n#how many words most and least common we would like\n#length of word_counter = 123645\nn_words = int(len(word_counter)*0.0001) \n\ntop_words = list_of_common_words(n_words)\n\nprint('list of most common words: ' , top_words)","execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"Here we use count_words() but convert the list of counter into a string of just the words that are most common.\nIts very useful to have the list of only the words as strings and not as type counter if we would want to use it to filter our data.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00018-9da26ae7-a2d1-4dd7-a98d-cfd52466b7a5"}},{"cell_type":"markdown","source":"### 4.\n**b. ** Use the parameters in Sklearn’s `CountVectorizer` to filter out these words. Update the program from point 3 and run it on your data and report your results.\n\nYou have two options to do this in Sklearn: either using the words found in part (a) or letting Sklearn do it for you. Argue for your decision-making.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00015-15190686-0764-4a54-953e-de6fdc2bb126","output_cleared":false}},{"cell_type":"markdown","source":"*** Using our own list of common words ***\n\n\nUsing our own list of most common words to filter the mails with could give us the best result for this specific\nscenario, which is often the case with data science where we have adapt methods to a specific problem. \nBut a disadvantage of this is that it takes longer to run and the program may be \ntoo specifically tailored trained to be used in more general cases.\nSomething elso to consider is that our list of most common words never take into account how long the emails are which mean longer emails weigh more than short ones.\nAnother problem that could appear is how many words are considered to be _common_.\nFrom this we therefore decide to use the sklearns filtering.\n\n\n*** Using Sklearn's filtering ***\n\nWe opted to use Sklearn's algorithm for filtering out the words with the main reasoning being the performance of the program. By using the built-in\nlimiter of frequency of words _max_df_ we can limit the words used in the emails more effectively, by not having too loop through the whole Counter\nant selecting only the words that apply to the condition we specified. Theoretically we should have the same results, since both the methods\nsimply takes the most common words. Bu the difference is that in using the Counter above, we can specify the filtering according to how our document\nlooks like. This means that the CountVectorizer may not filter out `\\n` and `\\t` which we specifically told the algorithm above to do. The problem \nmay be then that the Counter are dependent on that we know about the structure and data in the email, which we can't with this size of dataset. The\nmost logical solution is then to let the proven algorithm filter out tokens and words for what has been learnt about natural language and filtering\nduring development of these counters, comparing to us guessing to what words or tokens might be in the data.\n\nIt is worth to note that the results are dependent on more variables, not just if we use our own pre-preprocessor or Sklearn's. With the CountVectorizer\nwe can specify parameters such as `max_df`, `min_df`, `max_features`, `stop_words`, or `ngram_range` which alter furthermore what to filter out. In our\ncase we chose to specify only `max_df` and `stop_words`, so that we filter out the words that appear in 50% or more of the documents, and including\neastablish common words for Natural Language (such as _you_, _I_, or _the_).","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00019-5cf697e0-c82b-45cc-905c-5185640df654","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00018-95fa89c6-f873-448b-a831-21502d62ff2a","output_cleared":false,"source_hash":"c6058005","execution_millis":4607,"execution_start":1606921951700},"source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn import svm\n\n# Uncomment to download stopwords locally\n#nltk.download('stopwords')\n\n\n# Modified Naive Bayes with filtering\ndef NB_filter(dataframe):\n\n    X = dataframe['message']\n    y = dataframe['class']\n\n    #instantiate vectorize with custom stop words\n    #X_vectorized = CountVectorizer(stop_words=stop_words_top).fit_transform(X)\n    \n    # Filter out words with frequency of more than 50%, aswell as common english stopwords\n    X_vectorized = CountVectorizer(max_df=0.5, stop_words=stopwords.words('english')).fit_transform(X)\n\n    # Divide the emails into train and test sets\n    hamtrain, hamtest, spamtrain, spamtest = train_test_split(X_vectorized, y, test_size=0.25, random_state = 0)\n\n    # instantiate a Multinomial Naive Bayes model\n    mnb_classifier = MultinomialNB()\n    mnb_classifier.fit(hamtrain, spamtrain)\n    mnb_predictions = mnb_classifier.predict(hamtest)\n\n    # instantiate a Bernoulli Naive Bayes model\n    bnb_classifier = BernoulliNB()\n    bnb_classifier.fit(hamtrain, spamtrain)\n    bnb_predictions = bnb_classifier.predict(hamtest)\n\n    # Calc TP and FN rates\n    mnb_tp, mnb_fn = calc_rates(mnb_predictions,spamtest,hamtest)\n    bnb_tp, bnb_fn = calc_rates(bnb_predictions,spamtest,hamtest)\n\n    return mnb_tp, mnb_fn, bnb_tp, bnb_fn\n\n\ndf_easy_ham_spam = extract_mails(['./20021010_easy_ham.tar.bz2','./20021010_spam.tar.bz2'],['ham','spam'])\ndf_hard_ham_spam = extract_mails(['./20021010_hard_ham.tar.bz2','./20021010_spam.tar.bz2'],['ham','spam'])\n\n# Use easy_ham and hard_ham in filtering Naive Bayes\ne_mnb_tp, e_mnb_fn, e_bnb_tp, e_bnb_fn = NB_filter(df_easy_ham_spam)\nh_mnb_tp, h_mnb_fn, h_bnb_tp, h_bnb_fn = NB_filter(df_hard_ham_spam)\n\nprint('Spam vs. easy-ham with filtered df')\nprint('Multinomial Naive Bayes model gives TP rate = %f and FN rate = %f' %(e_mnb_tp,e_mnb_fn))\nprint('Bernoulli   Naive Bayes model gives TP rate = %f and FN rate = %f' %(e_bnb_tp,e_bnb_fn))\nprint()\nprint('Spam vs. hard-ham with filtered df')\nprint('Multinomial Naive Bayes model gives TP rate = %f and FN rate = %f' %(h_mnb_tp,h_mnb_fn))\nprint('Bernoulli   Naive Bayes model gives TP rate = %f and FN rate = %f' %(h_bnb_tp,h_bnb_fn))\n","execution_count":42,"outputs":[{"name":"stdout","text":"Spam vs. easy-ham with filtered df\nMultinomial Naive Bayes model gives TP rate = 0.922414 and FN rate = 0.077586\nBernoulli   Naive Bayes model gives TP rate = 0.456897 and FN rate = 0.543103\n\nSpam vs. hard-ham with filtered df\nMultinomial Naive Bayes model gives TP rate = 0.954198 and FN rate = 0.045802\nBernoulli   Naive Bayes model gives TP rate = 0.969466 and FN rate = 0.030534\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Above we see the results for using sklearn to filter out words with frequency of more than 50%, aswell as common english stopwords.\nCompared to unfiltered we see that the multinomial model gets better at predicting _spam vs.easy-ham_ but is as good as before to predict at _spam vs-hard-ham_.\nThe Bernoulli actually gets a bit worse at predictiong everything.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00022-049c037e-02e7-45de-8033-0df7314c9dc5"}},{"cell_type":"markdown","source":"### 5. Eeking out further performance\nFilter out the headers and footers of the emails before you run on them. \nThe format may vary somewhat between emails, which can make this a bit tricky,\n so perfect filtering is not required. Run your program again and answer the following questions: ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00013-33d8b44a-9086-418b-a477-84b154c08e16","output_cleared":false}},{"cell_type":"code","source":"# TODO: Filter headers and footer","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00022-23bb02c9-e895-4d6d-a93a-308d0d4b9a60"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 5.1 Does the result improve from 3 and 4? \n\n#### 5.2 The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies? \n\nThe result will be skewed because we will have inbalance in the data, where we might train on one class more (for example ham). \nWe noticed this when the results improved from task 2 to 3. But this can still yield\n\"good\" result when testing it, but if we would try to validate the data the model will perform worse. The model are predicting on different features\nthat may only exist in the test test, but not in the training set.\n\nA solution to this is to use stratification on the data, which locks the classes distributed in the training and testing sets. This can be done by using\nthe `stratify` parameter on `train_test_split`, and the model will then not have an imbalanced test set where the performance would seem to be good, but\nwhen acutally validating it, it will perform worse.\n\n#### 5.3 What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages? \n\nSince the model has then been trained on mainly features of spam emails, it will have a harder time to determine weather a ham message is ham or spam.\nAs mentioned in previous question, it may not be evident in the result becuase the training and testing are inbalanced, but would be evident when\nalso validating. Instead of looking at True Positive and False Negative we instead would want to look at False Positive and True Negative.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00022-80d3bf18-8c0f-489b-a276-387bccb3c317"}},{"cell_type":"markdown","source":"### Re-estimate your classifier using `fit_prior` parameter set to `false`, and answer the following questions:\n\n#### 5.4 What does this parameter (fit_prior) mean?\n\nHaving `fit_prior` set to true means that it includes previously known probabilities on the data, and this is the default that has been run up until \nthis question. When running it set to false, it will instead use a uniform prior which doesn't offer any regularization. This leads to infererence and\nundesired results. Uniform priros should not be used, unless we know that the bounds are representing true constraints. If we want to be vague about the prior\nit's better to not specify any at all. Alternatively we can use weak prios (bad-ish but not so bad as uniform). \n\n#### 5.5 How does this alter the predictions? Discuss why or why not.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00023-22fa3bf8-2d4f-4956-b144-34fc12e59154"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"c357d985-4588-4301-9455-87518b26bff9","deepnote_execution_queue":[{"cellId":"00017-5b62d8f8-a293-4f82-a028-6ecb412dd9d5","sessionId":"35d458c5-5f95-49e7-b2e0-e112fb070b42","msgId":"d63a938a-4cb0-4d8b-9237-deb8549a29eb"}]}}