{"cells":[{"cell_type":"markdown","source":"# Assignment 5: Reinforcement learning and Classification\n*Group 11: Alexandra Parkegren & Albin Sjöstrand*\n\n*Hours spent Alexandra: *\n\n*Hours spent Albin: *\n","metadata":{"id":"O73TxqjJH7e7","cell_id":"00000-65e373e8-388a-414b-8932-8ad6bd2d230d","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Primer\n\n## Decision Making\nThe problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\ntwo parts. First, how do we learn about the world? This involves both the\nproblem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\ncurrently know about the world, how should we decide what to do, taking into\naccount future events and observations that may change our conclusions?\nTypically, this will involve creating long-term plans covering possible future\neventualities. That is, when planning under uncertainty, we also need to take\ninto account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\nthings should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\nknown to produce good results and experiment with something new is known\nas the **exploration-exploitation dilemma**.\n\n## The exploration-exploitation trade-off\n\nConsider the problem of selecting a restaurant to go to during a vacation. Lets say the\nbest restaurant you have found so far was **Les Epinards**. The food there is\nusually to your taste and satisfactory. However, a well-known recommendations\nwebsite suggests that **King’s Arm** is really good! It is tempting to try it out. But\nthere is a risk involved. It may turn out to be much worse than **Les Epinards**,\nin which case you will regret going there. On the other hand, it could also be\nmuch better. What should you do?\nIt all depends on how much information you have about either restaurant,\nand how many more days you’ll stay in town. If this is your last day, then it’s\nprobably a better idea to go to **Les Epinards**, unless you are expecting **King’s\nArm** to be significantly better. However, if you are going to stay there longer,\ntrying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\nbetter food for the remaining time, while otherwise you will have missed only\none good meal out of many, making the potential risk quite small.","metadata":{"id":"m_6obY12H7e7","cell_id":"00002-3cd1d859-809d-4c16-af24-5d3e76f53422","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Overview\n* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n\n\n* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration.","metadata":{"id":"fC-SVg0ZH7e7","cell_id":"00003-ef5453a3-ec89-46a3-8cbe-646c8d74b202","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Markov Decision Process\n\nMarkov Decision Process (MDP) provides a mathematical framework for modeling sequential decision making under uncertainty. A MDP consists of five parts: the specific decision times, the state space of the environment/system, the available actions for the decision maker, the rewards, and the transition probabilities between the states.\n\n* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ for the current state and action, and the resulting next state\n* Transition probabilities $p(s'|s,a)$ that taking action $a$ in state $s$ will lead to state $s'$\n\nAt a given decision epoch $t$ and system state $s_t$, the decions maker, or *agent*, chooses an action $a_t$, the system jumps to a new state $s_{t+1}$ according to the transition probability $p(s_{t+1}|s_t,a_t)$, and the agent receives a reward $r_t(s_t,a_t,s_{t+1})$. This process is then repeated for a finite or infinite number of times.\n\nA *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_1 \\in S$, which is also known as the *value* for that state, \n\n$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \n\nwhere $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if you think all future rewards should count equally, you would use $\\gamma = 1$, while if you only care less about future rewards you would use $\\gamma < 1$. The expected total *discounted* reward becomes\n\n$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n\nNow, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^{\\pi^*}(s)$ for all $s\\in S$. That is\n\n$$V^{\\pi^*}(s) \\geq V^\\pi(s), s\\in S$$\n\nThe problem of finding the optimal policy is a _dynamic programming problem_. It turns out that a solution to the optimal policy problem in this context is the *Bellman equation*. The Bellman equation is given by\n\n$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n\nThus, it can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n\nA real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \n\nA major drawback of MDPs is called the \"Curse of Dimensionality\". MDPs unfortunately do not scale very well with increasing sets of states or actions.   \n","metadata":{"id":"KurOZxYjH7e7","cell_id":"00005-dc650636-0c34-401e-a412-e7cddb715d23","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Question 1\n\nIn this first question we work with the deterministic MDP, no code is necessary in this part.\n\nSetup:\n\n* The agent starts in state **S**\n* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n* Note, that you cannot move outside the grid, thus all actions are not available in every box.\n* When reaching **F**, the game ends (absorbing state).\n* The numbers in the boxes represent the rewards you receive when moving into that box. \n* Assume no discount in this model: $\\gamma = 1$\n\nThe reward of a state $r(s=(x, y))$ is given by the values on the grid:\n    \n| a| b |c |\n|-- |--|--|\n|-1    | 1|**F**|\n|0     |-1|     1|  \n|-1    | 0|    -1|  \n|**S**|-1|     1|\n\nLet $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n","metadata":{"id":"QWPya78-H7e7","cell_id":"00007-d9bdd270-35bf-4516-bb9b-d396771e1775","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\n**1a) What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. NESW will make a circle.**\n\nBest path: `EENNN`\n\nAlternative path: `EENNWNE`\n\nIt's unique in that it's the one path that doesn't have an negative sum, but instead 0, while also being the shortest. There is an alternative path\nthat has the same sum of 0, but takes on two additional moves in order to reach F.\n","metadata":{"tags":[],"cell_id":"00007-2632c80f-20a4-4aea-8b92-debe750d6e62","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\n**1b) What is the optimal policy (i.e. the optimal action in each state)?**\n\nPosition has the optimal action(-s) with the coherent reward points\n\n(0,0):  E  ||  N  = -1\n\n(0,1):  E = 1\n\n(0,2):  W  ||  N  = -1\n\n(1,0):  E  ||  N   = 0 (if we are allowed to return to start S is also a equally possible policy)\n\n(1,1):  E  ||  W  ||  N  ||  S  =-1\n\n(1,2):  N  ||  S = 1 \n\n(2,0):  E  ||  N  ||  S =-1\n\n(2,1):  E  ||  N   = 1\n\n(2,2):  N = 0\n\n(3,0):  E = 1\n\n(3,1):  E = 0\n\n(3,2): absorbing state have no possible actions\n\n","metadata":{"tags":[],"cell_id":"00007-1c4f9d95-3fba-4dc7-b8f0-fb944966755b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Value Iteration","metadata":{"id":"ZyQ7IatcH7e7","cell_id":"00008-9b3557d1-5985-4892-bc5e-eebc3fb23f43","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**1c) What is expected total reward for the policy in 1b)?**\n\nV(s) is the value you expect to obtain when you follow the current policy starting from state s\n\nFrom this given specific policy $\\pi$ we can compute the expected total reward by adding \nthe rewards. The value is the current reward (from the position we are standing at) \nplus the optimal actions reward, the value expected to obtain.\nThe new value from an action will be = (currentStateReward + 1 \\* nextStateReward) \nwhere gamma=1 (it'll care more about rewards earned immediately).\nThese values will be added up/summed at the end into V(s), which will represent the expected \ntotal reward when following the current policy sdtarting from state s.\n\nWe start at (0,0) and follow the optimal policy (see 1b). \nSome states have multiple path choices. \nWe follow all alternative paths until we reach the goal and then we'll be able \nto compare the sum value to decide which way is the most optimal.\nSome path cross their own way, we then ignore these since the total sum will not improve by this.\n\nThe expencted total reward for the best chosen path is: `0`. \n\nThe calculation for this was as follows\n\n(0,0): Action E, value = 0 - 1=-1\n\n(0,1): Action E, value = -1+ 1= 0\n\n(0,2): Action N, value = 1 - 1= 0\n\n(1,2): Action N, value = -1+ 1= 0\n\n(2,2): Action N, value = 1 + 0= 1\n\n(3,2) sumValue = 0\n\n\n\nHere are the other possible paths but with worse value:\n\n(0,0):N, (1,0):E, (1,1):E, (1,2):N, (2,2):N, (3,2) sumV=-1-1-1+0+1= -2\n\n(0,0):N, (1,0):E, (1,1):N, (2,1):E, (2,2):N, (3,2) sumV=-1-1-1+0+1= -2\n\n(0,0):N, (1,0):E, (1,1):N, (2,1):N, (3,1):E, (3,2) sumV=-1-1-1+0+1= -2\n\n(0,0):N, (1,0):N, (2,0):E, (2,1):E, (2,2):N, (3,2) sumV=-1-1-1+0+1= -2\n\n(0,0):N, (1,0):N, (2,0):E, (2,1):N, (3,1):E, (3,2) sumV=-1-1-1+0+1= -2\n\n(0,0):N, (1,0):N, (2,0):N, (3,0):E, (3,1):E, (3,2) sumV=-1-1-1+0+1= -2\n","metadata":{"tags":[],"cell_id":"00007-ec14459e-9162-4f34-a9f6-4170a0e6265d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Question 2\n\nFor larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation. The process of repeated application of the Bellman equation what we here call the _value iteration_ algorithm.","metadata":{"id":"5NfqElM_H7e7","cell_id":"00009-8c81eca1-ac3c-4bf2-812d-3c6c7245c082","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The value iteration algorithm practically procedes as follows:\n\n```\nepsilon is a small value, threshold\nfor x from i to infinity \ndo\n    for each state s\n    do\n        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n    end\n    if  |V_k[s]-V_k-1[s]| < epsilon for all s\n        for each state s,\n        do\n            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n            return π, V_k \n        end\nend\n\n```\n\n\n\n\n","metadata":{"id":"4qbn4HjqR2fA","cell_id":"00010-4a2aba5d-1c6b-4f82-b28a-8a1c9f47ea1e","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability of 0.8 that that action will be performed and a probability of 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state, that is, we stay on the grid), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n\n| | | |  \n|----------|----------|---------|  \n|0|0|0|\n|0|10|0|  \n|0|0|0|  \n\n\nteacher said:\n(0,1)\nAction **N**: 0.8(10) + 0.2(0) = 8\n(0,1)\nAction **N**: 0.8(0) + 0.2(10) = 2\n\n**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n\n| | | |  \n|----------|----------|---------|  \n|0|8|0|\n|8|2|8|  \n|0|8|0|  \n\n**Iteration 2**:  \n  \nStaring with cell (0,0) (lower left corner): We find the expected value of each move:  \nAction **S**: 0  \nAction **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \nAction **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \nAction **W**: 0\n\nHence any action between **E** and **N** would be best at this stage.\n\nSimilarly for cell (1,0):\n\nAction **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 \n(Action **N** is the maximizing action)  \n\n\n\n\nSimilar calculations for remaining cells give us:\n\n| | | |  \n|----------|----------|---------|  \n|5.76|10.88|5.76|\n|10.88|8.12|10.88|  \n|5.76|10.88|5.76|  \n","metadata":{"id":"K7hOzat7H7e8","cell_id":"00011-faf64806-c2aa-4043-b007-f73f0240f286","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\n\n**2a)** Implement the value iteration algorithm just described here in python, \nand show the converging optimal value function and the optimal policy for the above \n3x3 grid. Hint: use the pseudo-code above as a starting point, but be sure to explain \nwhat every line does.","metadata":{"id":"ccoMLc71H7e8","cell_id":"00012-b2c387b5-4f84-4ac6-9a8e-0936ae2c70e3","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\"show the converging optimal value function\"\n\n$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n\n\n\n\"show the optimal policy (that fulfills the bellman equation)\"\n\n$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n","metadata":{"tags":[],"cell_id":"00015-4cd7ed79-e0f8-414b-a4a1-02416587bad6","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-8cdf8374-d28c-4c49-8670-b473afc7a6a3","output_cleared":false,"source_hash":"6a3dd7cb","execution_millis":0,"execution_start":1607508401516,"deepnote_cell_type":"code"},"source":"import numpy as np\n\ndef getPossibleActions(env,currentState):\n\n    #four acitions/ moves of the agent: LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\n    possibleActions = []\n\n    #corresponding Action will give the neighbour with the corresponging rate\n    possibleSuccessorStates = []\n    possibleSuccessorReward = []\n\n    posX = currentState[0]\n    posY = currentState[1]\n\n    envXLength = env.shape[0]\n    envYLenght = env.shape[1]\n    \n    #if it can move to the left\n    if (posY-1) >= 0:\n        possibleActions.append(0)\n        possibleSuccessorStates.append([posX,posY-1])\n        possibleSuccessorReward.append(env[posX,posY-1])\n\n    #if the agent can move down\n    if (posX+1) <= (envXLength-1):\n        possibleActions.append(1)\n        possibleSuccessorStates.append([posX+1,posY])\n        possibleSuccessorReward.append(env[posX+1,posY])\n\n    #if the agent can move to the right\n    if (posY+1) <= (envYLenght-1):\n        possibleActions.append(2)\n        possibleSuccessorStates.append([posX,posY+1])\n        possibleSuccessorReward.append(env[posX,posY+1])\n\n    #if the agent can move up\n    if (posX-1) >= 0:\n        possibleActions.append(3)\n        possibleSuccessorStates.append([posX-1,posY])\n        possibleSuccessorReward.append(env[posX-1,posY])\n\n    return possibleActions, possibleSuccessorStates, possibleSuccessorReward\n\n\n\n# gamma ex = 0.9 will care more about rewards earned immediately rather than later.\n# there is a probability of probMove(ex 0.8) that that action will be performed and probStay (ex 0.2) that no action is taken\ndef getOptimalValue(env,currentState,probMove, probStay,gamma,eps,initalEnviroment):\n\n    currentReward = env[currentState]\n    possActions, possSuccStates, possSuccStatesRewards = getPossibleActions(env,currentState)\n\n    bestValue = 0;\n    \n    for i in range(0,len(possActions)):\n        if initalEnviroment:\n            #this is the way to count the value for the initial enviroment\n            newValue = probMove*(possSuccStatesRewards[i]) + probStay*(currentReward)\n        else:\n            #the optional policy V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n            newValue = probMove*(currentReward + gamma*possSuccStatesRewards[i]) + probStay*(currentReward + gamma*currentReward)\n\n        #the converging optimal value function π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′]))\n        #initialize for first loop or, if the newValues is better (within a epsilon range)\n        if (i == 0) or ((newValue > bestValue)): # and ((newValue - bestValue) < eps)):\n            bestValue = newValue   \n        \n    return bestValue\n    \n\n\ndef updateEnviroment(env,probMove,probStay,gamma,eps,n_wanted_updates,initalEnvIncluded):\n\n    #save the inital enviroment and all updated enviroments for return later\n    listOfEnviroments = [env]\n    \n    # one loop will create a new enviroment with values from the first\n    for i in range (0,n_wanted_updates): \n        \n        oldEnv = listOfEnviroments[i]\n        newEnviroment = np.zeros(env.shape)\n        \n        for x in range(0,env.shape[1]):\n            for y in range(0,env.shape[0]):\n                pos = (x,y)\n                \n                if initalEnvIncluded & (i==0):\n                    bValue = getOptimalValue(oldEnv,pos,probMove,probStay,gamma,eps,True)\n                else:\n                    bValue = getOptimalValue(oldEnv,pos,probMove,probStay,gamma,eps,False)\n                #make a enviroment with the best values\n                newEnviroment[pos] = bValue\n            \n        #save a new updated enviroment into an array to save the history\n        listOfEnviroments.append(newEnviroment)\n        \n    return listOfEnviroments\n\n\n\n\n#currentState = (0,0)\n#enviroment = np.array([[0,8,0],[8,2,8],[0,8,0]])\n#possibleActions, possibleSuccessorStates, possibleSuccessorReward = getPossibleActions(enviroment,currentState)\n#bestValue= getOptimalValue(enviroment,currentState,0.8,0.2,0.9,0.5)\n\n\n\nn_wanted_updates = 2\nenviroment = np.array([[0,0,0],[0,10,0],[0,0,0]])\nenviroments = updateEnviroment(enviroment,0.8,0.2,0.9,0.5,n_wanted_updates,True)\n\nfor i in range(0,len(enviroments)):\n    if i==0:\n        print('the initial enviroment: ')\n    else:\n        print('an updates enviroment: ')\n    print(enviroments[i])\n\n\n\nprint()\nn_wanted_updates = 30\nenviroment2 = np.array([[0,0,0],[0,10,0],[0,0,0]])\nenviroments2 = updateEnviroment(enviroment2,0.8,0.2,0.9,0.5,n_wanted_updates,True)\nprint('result of one enviroment updated %d number of times :' %(n_wanted_updates))\nprint(enviroments2[n_wanted_updates])\nenviroment3 = np.array([[0,0,10],[0,0,0],[0,0,0]])\nenviroments3 = updateEnviroment(enviroment3,0.8,0.2,0.9,0.5,n_wanted_updates,True)\nprint('result of another enviroment updated %d number of times :' %(n_wanted_updates))\nprint(enviroments3[n_wanted_updates])\n\n","execution_count":133,"outputs":[{"name":"stdout","text":"the initial enviroment: \n[[ 0  0  0]\n [ 0 10  0]\n [ 0  0  0]]\nan updates enviroment: \n[[0. 8. 0.]\n [8. 2. 8.]\n [0. 8. 0.]]\nan updates enviroment: \n[[ 5.76 10.88  5.76]\n [10.88  8.12 10.88]\n [ 5.76 10.88  5.76]]\n\nresult of one enviroment updated 30 number of times :\n[[6.06490857e+08 6.06491100e+08 6.06490857e+08]\n [6.06491100e+08 6.06491100e+08 6.06491100e+08]\n [6.06490857e+08 6.06491100e+08 6.06490857e+08]]\nresult of another enviroment updated 30 number of times :\n[[6.06490857e+08 6.06491100e+08 6.06491100e+08]\n [6.06485828e+08 6.06490857e+08 6.06491100e+08]\n [6.06436926e+08 6.06485828e+08 6.06490857e+08]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$.\n\nThe calculatons are divided into sequence of smaller subproblems:\nEvery different state will be considered separately.\nThe possible actions for different states are independent of each other.\n\n\nFor the first loop the initial value will be spread out into the matrix. \nThe first iteration is trivial since by then we've moved away from the initial value.\nAnd when for example not care how the enviroment was initialized (see the last two prints above)","metadata":{"tags":[],"cell_id":"00012-cf4f90f5-d97a-4b59-92f3-736a5c968c2f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Reinforcement Learning (RL)\nUntil now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(a,s,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\n\nSo far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\n\n$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(a,s,s') + \\gamma V^\\pi(s')]$$\n\nThe value function and the action-value function are directly related through\n\n$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\n\ni.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\n\n$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(a,s\n]\\,s') + \\gamma V^*(s')]$$\n\nand the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\n\n$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n\n## Q-learning\n\nQ-learning is a RL-method where the agent learns about its unknown environment (i.e. the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\n\n$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$ \n\nwhere $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there is sufficient exploration. While a constant $\\alpha$ generally does not guarantee us to reach true convergence, we keep it constant at $\\alpha=0.1$ for this assignment.\n\n## OpenAI Gym\n\nWe shall use already available simulators for different environments (worlds) using the popular OpenAI Gym library. It just implements [different types of simulators](https://gym.openai.com/) including ATARI games. Although here we will only focus on simple ones, such as the [Chain enviroment](https://gym.openai.com/envs/NChain-v0/) illustrated below.\n![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\nThe figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\n\n\n","metadata":{"id":"BQXoOa7LH7e8","cell_id":"00013-e4b01708-218e-427b-b386-8be38e80f882","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Question 3\nYou are to first familiarize with the framework using its [documentation](http://gym.openai.com/docs/), and then implement the Q-learning algorithm for the Chain enviroment (called 'NChain-v0') using default parameters. Finally print the $Q^*$ table at convergence. Convergence is **not** a constant value, rather a stable plateau with some noise. Take $\\gamma=0.95$. You can refer to the Q-learning (frozen lake) Jupyter notebook shown in class, uploaded on Canvas. Hint: start with a small learning rate.\n","metadata":{"tags":[],"cell_id":"00021-390f2fe1-58ff-4e20-a3d7-7c6e3f8f9e06","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00021-168dd3de-48b5-4bc1-83f9-8ece97a6707f","output_cleared":false,"source_hash":"e40cca51","execution_millis":1,"execution_start":1607517365204,"deepnote_cell_type":"code"},"source":"# For testing where convergence on chain\nval5a = 0\nval1b = 0\n\nfor n in range(0, 200):\n    val5a = val5a + 10 * 0.95**n\n    val1b = val1b + 2 * 0.95**n\n#    print('current val 5a %f, current val 1b %f ' %(val5a,val1b))\n\n#print('Total val 5a %f, current val 1b %f ' %(val5a,val1b))\n\nval = 0\nfor n in range(0, 200):\n    val = val + 10 * 0.95**n\n    #print('current val: ', val)\n\n#print('Total val:', val)","execution_count":141,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00022-1a04bb91-1674-4610-9327-3704a006464b","output_cleared":false,"source_hash":"95ce33c3","execution_millis":13421,"execution_start":1607518561543,"deepnote_cell_type":"code"},"source":"import gym\nimport random\n\nnum_episodes = 1000\ngamma = 0.9\nlearning_rate = 0.8\nepsilon = 1\n\nenv = gym.make('NChain-v0')\nenv.reset()\n\n# initialize the Q table\nQ = np.zeros((env.observation_space.n, env.action_space.n))\nexp_exp_tradeoff = random.uniform(0,1)\n\nfor _ in range(num_episodes):\n\tstate = env.reset()\n\tdone = False\n\twhile done == False:\n        # First we select an action:\n\t\tif random.uniform(0, 1) < epsilon: # Flip a skewed coin\n\t\t\taction = env.action_space.sample() # Explore action space\n\t\telse:\n\t\t\taction = np.argmax(Q[state,:]) # Exploit learned values\n        # Then we perform the action and receive the feedback from the environment\n\t\tnew_state, reward, done, info = env.step(action)\n        # Finally we learn from the experience by updating the Q-value of the selected action\n\t\tupdate = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n\t\tQ[state,action] += learning_rate*update \n\t\tstate = new_state\n\nprint('Q-table: ')\nprint(Q)","execution_count":162,"outputs":[{"name":"stdout","text":"Q-table: \n[[18.49086294 19.67473114]\n [22.11221996 19.76885434]\n [25.01656377 18.90197004]\n [20.80278135 27.90070762]\n [37.72279119 22.7879921 ]]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00022-5f632b81-ab2b-4581-b043-d71bb5129024","output_cleared":false,"source_hash":"a91032b9","execution_millis":13134,"execution_start":1607518616244,"deepnote_cell_type":"code"},"source":"import gym\nimport random\nimport numpy as np\n\n# init parameters for this scenario\nlearning_rate = 0.8\ndiscount_factor = 0.9\nepsilon = 1 # exploration rate\nmin_epsilon = 0.1 # minimum exploration rate\nmax_epsilon = 1 # maximum exploration rate\ndecay = 0.01 #e-greedy -> minize the exploration rate for each episode\ntrain_episodes = 1000\nmax_steps = 10\n\n# init environment\nenv = gym.make('NChain-v0')\nenv.reset()\n\n# init Q table\nQ = np.zeros((env.observation_space.n, env.action_space.n))\n\n# store results\ntraining_rewards = []\nepsilons = []\n\n# train the algorithm with specified no of training episodes\nfor episode in range(train_episodes):\n    state = env.reset()\n    total_training_rewards = 0\n    done = False\n\n    #for step in range(max_steps):\n    while done == False:\n\n        # First we select an action:\n        # exploit (aldready something we know) or explore (new)\n        exp_exp_tradeoff = random.uniform(0,1)        \n        if exp_exp_tradeoff > epsilon:\n            action = np.argmax(Q[state,:])     # Exploit learned values\n        else:\n            action = env.action_space.sample() # Random explore action space\n\n        # action is taken new state, reward and criteria for stopping is observed\n        new_state, reward, done, info = env.step(action)\n\n        # update Q table with current state, learning rate, reward, discount, action taken and if the enivorment is done\n        update = reward + (discount_factor * np.max(Q[new_state, :])) - Q[state, action]\n        Q[state, action] += update * learning_rate\n\n        #Q[state, action] = Q[state, action] + learning_rate * (reward + discount_factor * np.max(Q[new_state, :])) - Q[state, action]\n\n        total_training_rewards += reward\n        state = new_state\n\n        # check is environment is done\n        #if done == True:\n            #print(\"Total reward for episode {}: {}\".format(episode, total_training_rewards))\n            #break\n    \n    # epsilon is updated for the exploitation exploration trade-off, e-greedy algorithm\n    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n    training_rewards.append(total_training_rewards) # total rewards\n    epsilons.append(epsilon) # save epsilon values\n\nprint('Q-table:')\nprint(Q)","execution_count":164,"outputs":[{"name":"stdout","text":"Q-table:\n[[10.52299248 17.69315329]\n [10.22354802 12.09274992]\n [17.8139887  11.22472349]\n [19.34880604 13.42212144]\n [18.94470438 13.51436317]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Question 4\n\n**4a) Define the MDP corresponding to the Chain environment above and verify that the optimal $Q^*$ value obtained using simple Q-learning is the same as the optimal value function $V^*$ for the corresponding MDP's optimal action. Hint: compare values obtained using value iteration and Q-learning.**\n\nTraversing chain. Big reward at the end.","metadata":{"tags":[],"cell_id":"00023-f56a179c-e4a2-4240-9b09-3de43f99356c","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import numpy as np\n\n\n\ndef createEnviroment():\n    # for corresponding state the action will give [succState, succReward]\n    #states = np.array([[1],[2],[3],[4],[5]])\n    successorStatesWithReward = {'a':[2,0],'b':[1,2]},{'a':[3,0],'b':[1,2]},{'a':[4,0],'b':[1,2]},{'a':[5,0],'b':[1,2]},{'a':[5,10],'b':[1,2]}\n    enviroment = np.zeros((5,2))\n    return enviroment, successorStatesWithReward\n\n\n\ndef getSuccRewardFromAction(succStatesRewards, currentState, action):\n    actionStateRew = succStatesRewards[currentState]\n    succStateWithReward = actionStateRew[action]\n    #print('the state %d has the possible actions %s and will make the action %s' %(currentState,actionStateRew,action))\n    #succState  = succStateWithReward[0]\n    succReward = succStateWithReward[1]\n    return succReward\n\n\ndef getOptimalValue11(succStatesRewards,currentState):\n    bestSuccReward = 0\n    currentStatesPossActions = succStatesRewards[currentState]\n    succstateA,succRewardA = currentStatesPossActions['a']\n    succstateB,succRewardB = currentStatesPossActions['b']\n    if succRewardA > succRewardB:\n        bestSuccReward = succRewardA\n    else:\n        bestSuccReward = succRewardB       \n    return bestSuccReward\n    \n\n\n\ndef updateEnviroment(env,succStatesRewards,gamma,learning_rate,n_wanted_updates):\n    #save the inital enviroment and all updated enviroments for return later\n    listOfEnviroments = [env]\n    possibleActions = ['a','b']\n    # one loop will create a new enviroment with values from the first\n    for i in range (0,n_wanted_updates): \n        oldEnviroment = listOfEnviroments[i]\n        newEnviroment = np.zeros(env.shape)\n        for state in range(0,5):\n            for a in range(0,2):\n                action = possibleActions[a]\n                succReward = getSuccRewardFromAction(succStatesRewards, state, action)\n                bestSuccReward = getOptimalValue11(succStatesRewards,state)\n                currentReward = oldEnviroment[state,a]\n                # Q-learning formula\n                newValue = currentReward + (learning_rate*(succReward + (gamma*bestSuccReward) - currentReward))\n                #update with new reward\n                newEnviroment[state,a] = newValue\n        #save a new updated enviroment into an array to save the history\n        listOfEnviroments.append(newEnviroment)     \n    return listOfEnviroments\n\n\n\ngamma = 0.5  #\"discount_factor\"\nn_wanted_updates = 4\nlearning_rate = 0.8\n\nenv, succStatesRewards = createEnviroment()\n#print('state %d can take action to get to states with rewards %s: ' %(env[0],succStatesRewards[0]))\n#print('first state take action a and go to state %d, to get reward %d' %(succStatesRewards[0]['a'][0],succStatesRewards[0]['a'][1]))\n\nlistOfEnv = updateEnviroment(env,succStatesRewards,gamma,learning_rate,n_wanted_updates)\n\nfor i in range(0,len(listOfEnv)):\n    print(listOfEnv[i])\n\n########\n","metadata":{"tags":[],"cell_id":"00022-39ff9338-6580-4112-8f96-efe90308355c","output_cleared":false,"source_hash":"e0893cde","execution_millis":5,"execution_start":1607519196194,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[[0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]]\n[[ 0.8  2.4]\n [ 0.8  2.4]\n [ 0.8  2.4]\n [ 0.8  2.4]\n [12.   5.6]]\n[[ 0.96  2.88]\n [ 0.96  2.88]\n [ 0.96  2.88]\n [ 0.96  2.88]\n [14.4   6.72]]\n[[ 0.992  2.976]\n [ 0.992  2.976]\n [ 0.992  2.976]\n [ 0.992  2.976]\n [14.88   6.944]]\n[[ 0.9984  2.9952]\n [ 0.9984  2.9952]\n [ 0.9984  2.9952]\n [ 0.9984  2.9952]\n [14.976   6.9888]]\n","output_type":"stream"}],"execution_count":166},{"cell_type":"code","source":"\ndef createEnviroment():\n    # for corresponding state the action will give [succState, succReward]\n    #states = np.array([[1],[2],[3],[4],[5]])\n    successorStatesWithReward = {'a':[2,0],'b':[1,2]},{'a':[3,0],'b':[1,2]},{'a':[4,0],'b':[1,2]},{'a':[5,0],'b':[1,2]},{'a':[5,10],'b':[1,2]}\n    enviroment = np.zeros((5,2))\n    return enviroment, successorStatesWithReward\n\ndef getSuccRewardFromAction(succStatesRewards, currentState, action):\n    actionStateRew = succStatesRewards[currentState]\n    succStateWithReward = actionStateRew[action]\n    #print('the state %d has the possible actions %s and will make the action %s' %(currentState,actionStateRew,action))\n    #succState  = succStateWithReward[0]\n    succReward = succStateWithReward[1]\n    return succReward\n\ndef getAllSuccRewardForState(succStatesRewards, currentState):\n    possabilitiesForState = succStatesRewards[currentState]\n    possRewards = [0,0]\n    possActions = ['a','b']\n    for i in range(0,2):\n        action = possActions[i]\n        stateAndReward = possabilitiesForState[action]\n        possRewards[i] = stateAndReward[1] \n    return possRewards\n    \n\n\n# gamma ex = 0.9 will care more about rewards earned immediately rather than later.\n# there is a probability of probMove(ex 0.8) that that action will be performed and probStay (ex 0.2) that no action is taken\ndef getOptimalValue(env,succStatesRewards,pos,probMove,probStay,gamma,eps,initalEnviroment):\n    \n    action = possibleActions[pos[1]]\n    state = pos[0]\n    print('the state is ', state)\n    print('the action is', action])\n    \n    possActions = ['a','b']\n    currentReward = env[pos]\n    possSuccStatesRewards = getAllSuccRewardForState(succStatesRewards, currentState)\n\n\n    #bestSuccReward = 0\n    #if possSuccStatesRewards[0] > possSuccStatesRewards[1]:\n    #    bestSuccReward = possSuccStatesRewards[0]\n    #else:\n    #    bestSuccReward = possSuccStatesRewards[1]      \n   \n\n    bestValue = 0;\n    for i in range(0,len(possActions)):\n        print('the action ')\n        if initalEnviroment:\n            #this is the way to count the value for the initial enviroment\n            newValue = probMove*(possSuccStatesRewards[i]) + probStay*(currentReward)\n        else:\n            #the optional policy V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n            newValue = probMove*(currentReward + gamma*possSuccStatesRewards[i]) + probStay*(currentReward + gamma*currentReward)\n        print('one value is : ',newValue)\n        #the converging optimal value function π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′]))\n        #initialize for first loop or, if the newValues is better (within a epsilon range)\n        if (i == 0) or ((newValue > bestValue)): # and ((newValue - bestValue) < eps)):\n            bestValue = newValue   \n        \n    return bestValue\n       \ndef getOptimalValue111(env,succStatesRewards,pos,probMove,probStay,gamma,eps,initalEnviroment):\n    #save the inital enviroment and all updated enviroments for return later\n    listOfEnviroments = [env]\n    possibleActions = ['a','b']\n    \n    oldEnviroment = listOfEnviroments[i]\n    newEnviroment = np.zeros(env.shape)\n    for state in range(0,5):\n        for a in range(0,2):\n            action = possibleActions[a]\n            succReward = getSuccRewardFromAction(succStatesRewards, state, action)\n            bestSuccReward = getOptimalValue11(succStatesRewards,state)\n            currentReward = oldEnviroment[state,a]\n            # Q-learning formula\n            newValue = currentReward + (learning_rate*(succReward + (gamma*bestSuccReward) - currentReward))\n            #update with new reward\n            newEnviroment[state,a] = newValue\n    #save a new updated enviroment into an array to save the history\n    listOfEnviroments.append(newEnviroment)     \n    return listOfEnviroments\n\n\ndef updateEnviroment(env,succStatesRewards,probMove,probStay,gamma,eps,n_wanted_updates,initalEnvIncluded):\n\n    #save the inital enviroment and all updated enviroments for return later\n    listOfEnviroments = [env]\n\n    # one loop will create a new enviroment with values from the first\n    for i in range (0,n_wanted_updates): \n        \n        oldEnv = listOfEnviroments[i]\n        newEnviroment = np.zeros(env.shape)\n        print('heeej',env.shape)\n        for x in range(0,env.shape[0]):\n            for y in range(0,env.shape[1]):\n                pos = (x,y)\n                if initalEnvIncluded & (i==0):\n                    bValue = getOptimalValue(oldEnv,succStatesRewards,pos,probMove,probStay,gamma,eps,True)\n                else:\n                    bValue = getOptimalValue(oldEnv,succStatesRewards,pos,probMove,probStay,gamma,eps,False)\n                #make a enviroment with the best values\n                newEnviroment[pos] = bValue\n            \n        #save a new updated enviroment into an array to save the history\n        listOfEnviroments.append(newEnviroment)\n        \n    return listOfEnviroments\n\nn_wanted_updates = 1\nenv, succStatesRewards = createEnviroment()\nenviroments = updateEnviroment(env,succStatesRewards,0.8,0.2,0.9,0.5,n_wanted_updates,True)\n\nfor i in range(0,len(enviroments)):\n    print(enviroments[i])\n\n","metadata":{"tags":[],"cell_id":"00023-508ae4cb-f784-4183-a86d-e4af11e5f712","output_cleared":false,"source_hash":"de848193","execution_millis":11,"execution_start":1607523606911,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"heeej (5, 2)\nthe state is  0\nthe action is a\nthe action \none value is :  0.0\nthe action \none value is :  1.6\nthe state is  0\nthe action is b\nthe action \none value is :  0.0\nthe action \none value is :  1.6\nthe state is  1\nthe action is a\nthe action \none value is :  0.0\nthe action \none value is :  1.6\nthe state is  1\nthe action is b\nthe action \none value is :  0.0\nthe action \none value is :  1.6\nthe state is  2\nthe action is a\nthe action \none value is :  0.0\nthe action \none value is :  1.6\nthe state is  2\nthe action is b\nthe action \none value is :  0.0\nthe action \none value is :  1.6\nthe state is  3\nthe action is a\nthe action \none value is :  0.0\nthe action \none value is :  1.6\nthe state is  3\nthe action is b\nthe action \none value is :  0.0\nthe action \none value is :  1.6\nthe state is  4\nthe action is a\nthe action \none value is :  0.0\nthe action \none value is :  1.6\nthe state is  4\nthe action is b\nthe action \none value is :  0.0\nthe action \none value is :  1.6\n[[0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]]\n[[1.6 1.6]\n [1.6 1.6]\n [1.6 1.6]\n [1.6 1.6]\n [1.6 1.6]]\n","output_type":"stream"}],"execution_count":206},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00022-a7664854-d3ab-4200-819b-aac98dcba181","output_cleared":false,"source_hash":"1102d888","execution_millis":4,"execution_start":1607440625439,"deepnote_cell_type":"code"},"source":"print(env.compute_reward(achieved_goal=True))","execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"compute_reward() missing 2 required positional arguments: 'desired_goal' and 'info'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-292-b45b93a36bd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0machieved_goal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: compute_reward() missing 2 required positional arguments: 'desired_goal' and 'info'"]}]},{"cell_type":"markdown","source":"**4b) What is the importance of exploration in RL? Explain with an example.**\n\nExploration is important to not focus on specific areas of the data, and where we could otherwise end up with local optimums that are according to the\nalgorithm great but may not apply to other parts of the data. If we take a real life example when choosing cereal, where you have chosen cereal a multiple times\nand know that you like them. You would continue to chose this cereal, **exploiting** the fact that you know that they're tasty. But you haven't explored\nany other options, and maybe cereal B would be tastier, healthier, or cheaper? In order for you to find out, you have to **explore** new options. You want\nwhat is best for you in the long run, not just in the moment, and it's the same for the algorithm. We can take new actions that can lead to a better overall \noutcome.","metadata":{"tags":[],"cell_id":"00026-12d1b0ce-25bd-415e-8d25-aa9da5606927","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Question 5\n\n**5a) Give a summary of how a decision tree works and how it extends to random forests.**\n\nA decision tree is a class of regression and classification methods that can be represented with a tree structured graph where nodes represent\nquestions, branches represent answers, and leaves represent labels. The first question is asked at the root of the tree, and for each level a new \nquestion is asked. Everytime a question is asked, a new branch is added and a new area in the feature space is split off. We do this until a\nset number of splits has been achieved, and where we don't want to split if it doens't improve the impurity (homogenous).\n\nRandom Forests are a combination of decision trees but with more flexibility. So it's based on a decision tree, but the big difference is that a single\ndecision tree does not determine the answer but rather the whole forest. Then the majority of the vote determine the answer, and generally we then\nminimize the variance of predictions since each decision tree are better or worse at specific elements of decisions. So we ask multiple decision trees\nthe same question and then take the majority as the answer.\n\n**5b) Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification.**\n\nIn RL the agent doens't usually know the environment, but can interact with it to learn and improve from it. Compared to regression or classification\nwhere we need to have a previous understanding of the environment in order to get the optimal performance. By instead using RL we can figure out the\nbest results by trial and error. RL is also more independent than supervised learning methods (as the name suggests) where less tweaking by the user\nneeds to be done since the algorithm can learn from misstakes by themselves. Supervised learning also doesn't include exploration, since it's given\nthe dataset and the algorithm doesn't need to collect the data. It also typically only makes one decision, \"is this image a chihuahua or muffin?\", and\nit will only learn if those decisions were right later.","metadata":{"id":"UWUqaN60H7e8","cell_id":"00014-f2d57912-98f6-4ea0-ba2e-96f12249cba6","output_cleared":false,"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\n# References\nPrimer/text based on the following references:\n* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf","metadata":{"id":"Wev-_UhcH7e8","cell_id":"00015-3a999aba-7d5c-4e28-99b0-e80e2fdffc56","output_cleared":false,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Assignment5_Reinforcement_Learning.ipynb","provenance":[],"collapsed_sections":[]},"deepnote_notebook_id":"6c7454da-3e81-41c9-b4a2-96d72b097583","deepnote_execution_queue":[]}}