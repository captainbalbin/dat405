`*` = unsure answers

# Part 1

## 1.

- Increasing computational power
- Availability of personal data on the internet and other places

## 2.

- Protecting the privacy of subjects is a matter of safeguarding their dignity and welfare.
- A failure to protect the privacy of subjects could render them vulnerable to identity theft and extortion `*`

## 3.

22,3,5

- In personal health records including hospital admission removing names, blood type, annual income, information about HIV status, and social security numbers of subjects.
- A series of street-level cameras in down-town Gothenburg are set up to disincentivize crime and simultaneously to collect data to reduce crowding and improve citizens shopping experiences. The cameras use a facial recognition algorithm to track pedestrians faces and count how often subjects pass by a given camera. No images are stored, each camera generates a coded representation of each face using the same algorithm on each camera. The coded representation of a face, along with the GPS coordinates of the camera, date, and time are stored.

## 4.

- Tell us whether a sample is likely to have occurred under the null-hypothesis.
- Infer the underlying cause of the measured data.

## 5.

- Eugenics
- Biological evolution

## 6.

- survey data
- Person made use of teachers assessment of student intelligence.
- Pearson did not account for environmental factors that could not be improved, e.g. the students’ refugee status. `*`

## 7.

- Yes, because the challenge includes predicting the future (“wave 6”), and only selected data is available for the time-point (“wave 6”) to be predicted- `*`
- Yes, because social outcomes depends on many parameters. Many of which may not be captured well by the dataset. `*`

## 8.

- Descriptions
- The current understanding is correct but incomplete because it lacks theories that explain why outcomes are difficult to predict even with high-quality data.
- Causal inference.

## 9.

- Using predictive modeling in the criminal legal system may lead to wrongful con- victions and arrests. `\*``
- Using predictive modeling in the child-protective services may lead to wrongful removal of children from their families. `*`

## 10.

- Sampling bias `*`
- Systemic bias in society `*`
- Poorly designed surveys `*`

# Part 2

## 1.

- When collecting the data internally they had very low diversity and a small sample size, with only three white males in a small age range. Which means that the predictions algorithm have been trained on a small set and specific data, that when it would be used in public testing the predictions might not be correct since other factors can affect it, for example other genders and different age groups. These predictions can then cause problems since the predictions are meant to help insurence providers, where a wrongful prediction can have big effects on what people have to pay for insurance.

- Yes, they should do the initial testing with a more diverse sample, to include different aspect that might affect insurance policies such as different genders, age groups, or consider including people with known diagnosis. With better data the predictions will be more accurate when it would be deployed since it would've been training on the right type of data.

## 2.

- Yes, because using a program where it is known that only three male caucasians is the base for what is considered healthy and not considering any other aspects would not be seen as fair by customers. If they think using the problem is fair, then they're basically saying that they think it's fair to only compare the health of people to three white males in a more prime age span.

# Part 3

## 1.

- Phones have different cameras with different qualities, as well are dependent on the lighting and angles that can alter a look. It's unknown what they AI consider to be a 'criminal look' since it very much depends on the training samples. It might also predict differently depending on in which country it's used with different features.

(filters are known to have 'whitened' people with beauty filters)

- Yes. The app is most likely marketed as a 'selfie'-filter where users are believed to think that it's only used for creating filters for funny posts. When that company then sells the data it's using they use their customers by conceiveing them with what it's for. It also depends on how the accurate the predictions are working. Since in this case it seems that the law-enforcement are given specific people just based on selfies and like, does not necessarily mean that they actually will protest. It also concerns with picking specific people from a crowd that you have to proof of actions what they've done, and pre-judge them without a real basis of concern.
