

# Part 2

## 1a. How can the training pipeline affect the predictions, of Vivaksa’s health status prediction algorithm, when deployed on the general public?

When collecting the data internally they had extremely low diversity and a small sample size, with only three white males in a short age range, which means that the predictions algorithm have been trained on a small set and specific data. 
The predictions will definitely not represent the general public and therefore might be misleading.
The training of the algorithm does not account for other factors, such as people with different genders, ages, ethnicity, living area, wages, educations and living habits.
This can then result in people getting charged wrongly because of an inaccurate prediction algorithm.

It is also important to understand that this data and its predictions can not fully grasp everything that has to do with a persons future insurance involvement.


## 1b. Can Vivaksa improve their predictions? if so how? and if not, why?


Yes, they can improve their predictions by having a much larger dataset to base their predictions on, aswell as including more diverse subjects of different ages and genders. 
Apart from the smart watches, blood and saliva tests they could also include additional features that determine whether a person is considered healthy or not, such as medical history, known diagnoses and diseases, genetics, diet, surronding and personal interests. 
This could make a big difference if one likes going to the gym or doing extreme sports. 

The predictions can further be improved by testing during longer periods of time to minimise the risks of outliers in the data.

They should also include margin errors for the tests that they are conducting and make sure they know how to use their devices correctly.



## 2. Can the roll-out of this pilot program undermine the insurance company’s own value of fairness? – explain why or why not.


- Yes, because using a program where it is known that only three male caucasians is the base for what is considered healthy and 
not considering any other aspects would not be seen as fair by customers. 
If they think using the problem is fair, then they're basically saying that they  think it's fair to only compare the health of
 people to three white males in a more prime age span.
It could be positive for people that are healthier than what the statistics where, which will lower their premium.
The ones who do not gain anything from using the app will not use it, which in turn would shift the value of fairness as well.
Users could also cheat the system to seem more healthy than they are to lower their insurance premium, shich would not be fair to those taking it seriously.
The users of the program might also use the app and the smart watches wrong, or that the people are using different 
devices that registers data differently.


To increase the premium only for the people who will use the app will exclude all that can not byt the device that can use 
the app, and also the people who uses other type of devices not supporting the app.


# Part 3

## 1.What are the limitations of Selfie2Personalitys technology - if any? 

The technology of image analysis is still under constuction which means we can not completely rely on it yet,
and since the quality of the images will differ alot, even a good algorithm will be difficult to trust.

The company needs to be transparent about what their techology do.
Knowing that the app search for criminal behaviour, some users might adapt so that they are perceived differently, 
manipulation the technology of how they are perceived.

It's unknown what they AI consider to be a 'criminal look' since it very much depends on the training samples. 
It might also predict differently depending on different ethnicity with different features.

Another limitation is that humans need to be involved in controlling for race, gender, and age. The conclusions
are then determined by humans who may be biased or have own opinions that affects the outcome.



## 2. Are there any ethical and privacy concerns with this collaboration and how may they manifest? 

- Yes. 
The app is most likely marketed as a 'selfie'-filter where users are believed to think that it's only 
used for creating filters for funny posts, while the company then sells the data and deceiving users.

Users should also be concered about what data is collected and how much data. The company might state that they are 
collecting data to 'improve' the app when in fact they are creating a more detailed profiles of users that they can sell
for other purposes one didn't sign up for.

It's also a concern depending on what age the users are, that minors may use the app and epending on country there are 
laws that prevent companies from collecting data and information about children.

If the law-enforcement are given the names of specific people based on selfies and likes and predicted interests they 
should not pre-judge them without real evidence and proof of them taking any action.
Creating an algorithm will always contain some biases wich will predict accordingly.