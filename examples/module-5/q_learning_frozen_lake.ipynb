{"cells":[{"cell_type":"markdown","source":"# Q-learning with FrozenLake","metadata":{"cell_id":"00000-ebfa9c03-907e-4c82-9164-bd1ba5b92e0a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Based on https://github.com/ioarun/openai-gym/blob/master/frozenlake/frozenlake-qlearning.py \n\nEnvironment: https://gym.openai.com/ \n\nDetails: https://www.kaggle.com/sandovaledwin/q-learning-algorithm-for-solving-frozenlake-game/code","metadata":{"cell_id":"00001-580ed90e-0941-42af-8765-69bee8936276","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00002-54fd0d91-778b-4ef6-a4dc-261d51b214d1","deepnote_cell_type":"code"},"source":"#Note. You need to install gym! Sometimes difficult on Windows. Google for advise.\nimport gym\nimport numpy as np\nimport random\nimport math","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Problem description","metadata":{"cell_id":"00003-3c87bd90-39f4-4c84-bf1d-d377f2f62884","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00004-41134f11-1107-43d2-8dff-68459ded4b15","deepnote_cell_type":"code"},"source":"'''\nThe agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n\nA frozenlake-v0 is a 4x4 grid world that looks as follows:\nSFFF       \nFHFH       \nFFFH       \nHFFG       \n\nMeaning of the letters:\nS: starting point, safe\nF: frozen surface, safe\nH: hole, fall to your doom\nG: goal, where the frisbee is located\n\nThe 16 states (position of the agent): \nState 0: upper left corner (Start)\n...\nState 15: Lower right corner (Goal)\n\nThe 4 actions (moves of the agent):\nLEFT = 0,\nDOWN = 1,\nRIGHT = 2,\nUP = 3.\n\nReward:\nThe episode ends when you reach the goal or fall into the water. \nYou receive a reward of 1 if you reach the goal, and 0 otherwise.\n\nEffect of actions:\n        def inc(row, col, a):\n            if a == LEFT:\n                col = max(col-1,0)\n            elif a == DOWN:\n                row = min(row+1,nrow-1)\n            elif a == RIGHT:\n                col = min(col+1,ncol-1)\n            elif a == UP:\n                row = max(row-1,0)\n            return (row, col)\n'''","execution_count":null,"outputs":[{"data":{"text/plain":"'\\nThe agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\\n\\nA frozenlake-v0 is a 4x4 grid world that looks as follows:\\nSFFF       \\nFHFH       \\nFFFH       \\nHFFG       \\n\\nMeaning of the letters:\\nS: starting point, safe\\nF: frozen surface, safe\\nH: hole, fall to your doom\\nG: goal, where the frisbee is located\\n\\nThe 16 states (position of the agent): \\nState 0: upper left corner (Start)\\n...\\nState 15: Lower right corner (Goal)\\n\\nThe 4 actions (moves of the agent):\\nLEFT = 0,\\nDOWN = 1,\\nRIGHT = 2,\\nUP = 3.\\n\\nReward:\\nThe episode ends when you reach the goal or fall into the water. \\nYou receive a reward of 1 if you reach the goal, and 0 otherwise.\\n\\nEffect of actions:\\n        def inc(row, col, a):\\n            if a == LEFT:\\n                col = max(col-1,0)\\n            elif a == DOWN:\\n                row = min(row+1,nrow-1)\\n            elif a == RIGHT:\\n                col = min(col+1,ncol-1)\\n            elif a == UP:\\n                row = max(row-1,0)\\n            return (row, col)\\n'"},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","source":"## Define environment","metadata":{"cell_id":"00005-a5940a53-fe49-41de-8441-72e696427085","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00006-6a31bfa4-e55d-48c1-9112-2b4439b5ddbd","deepnote_cell_type":"code"},"source":"env = gym.make(\"FrozenLake-v0\",is_slippery=False)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00007-609081e3-618e-4d23-82b7-c722bf72ae95","deepnote_cell_type":"code"},"source":"env.render()","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n"}]},{"cell_type":"markdown","source":"## Actions","metadata":{"cell_id":"00008-462a5b8a-43b3-45a5-b331-9a829c3736af","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00009-caec60fe-03a9-45eb-bba9-662626681146","deepnote_cell_type":"code"},"source":"#Sample actions for exploration:\nenv.action_space.sample()","execution_count":null,"outputs":[{"data":{"text/plain":"1"},"execution_count":5,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","source":"## Initialization","metadata":{"cell_id":"00010-d8c74f1e-80b8-4c68-9cd5-6b7d5a6b82a8","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00011-7459822b-f047-4b0d-9617-63702d07d3c5","deepnote_cell_type":"code"},"source":"num_episodes = 15000 #20000 #60000\ngamma = 0.95 #0.99\nlearning_rate = 0.7 #0.95 #0.85\nepsilon = 0.5#1 #0.15 #0.1\n\n# initialize the Q table\nQ = np.zeros([16, 4])\nQ","execution_count":null,"outputs":[{"data":{"text/plain":"array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]])"},"execution_count":6,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","source":"## Training the Q-table","metadata":{"cell_id":"00012-bd75603b-315a-495f-8671-4413e9e4bec6","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00013-ffea534f-5338-47b2-8bf6-2cf4eb3ea108","deepnote_cell_type":"code"},"source":"for _ in range(num_episodes):\n\tstate = env.reset()\n\tdone = False\n\twhile done == False:\n        # First we select an action:\n\t\tif random.uniform(0, 1) < epsilon: # Flip a skewed coin\n\t\t\taction = env.action_space.sample() # Explore action space\n\t\telse:\n\t\t\taction = np.argmax(Q[state,:]) # Exploit learned values\n        # Then we perform the action and receive the feedback from the environment\n\t\tnew_state, reward, done, info = env.step(action)\n        # Finally we learn from the experience by updating the Q-value of the selected action\n\t\tupdate = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n\t\tQ[state,action] += learning_rate*update \n\t\tstate = new_state","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00014-573e2c30-69ac-4343-8fb7-b43638fbd84e","deepnote_cell_type":"code"},"source":"Q","execution_count":null,"outputs":[{"data":{"text/plain":"array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n       [0.73509189, 0.        , 0.81450625, 0.77378094],\n       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n       [0.81450625, 0.        , 0.77378094, 0.77378094],\n       [0.77378094, 0.81450625, 0.        , 0.73509189],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.9025    , 0.        , 0.81450625],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.81450625, 0.        , 0.857375  , 0.77378094],\n       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n       [0.857375  , 0.95      , 0.        , 0.857375  ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.9025    , 0.95      , 0.857375  ],\n       [0.9025    , 0.95      , 1.        , 0.9025    ],\n       [0.        , 0.        , 0.        , 0.        ]])"},"execution_count":8,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","source":"## Sanity check","metadata":{"cell_id":"00015-d78e6248-4ce8-4a6e-a31b-0789e3fb8484","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00016-617aeec5-6d1a-4fd7-baf7-4376cd9eb70f","deepnote_cell_type":"code"},"source":"'''\nLet us sanity check some of the Q-values. \nFirst we recall what the environment looks like:\nSFFF       \nFHFH       \nFFFH       \nHFFG       \n\nAnd what the 4 actions are:\nLEFT = 0\nDOWN = 1\nRIGHT = 2\nUP = 3\n'''","execution_count":null,"outputs":[{"data":{"text/plain":"'\\nLet us sanity check some of the Q-values. \\nFirst we recall what the environment looks like:\\nSFFF       \\nFHFH       \\nFFFH       \\nHFFG       \\n\\nAnd what the 4 actions are:\\nLEFT = 0\\nDOWN = 1\\nRIGHT = 2\\nUP = 3\\n'"},"execution_count":10,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"cell_id":"00017-b9c6062f-7f32-47cc-876d-cf785563da29","deepnote_cell_type":"code"},"source":"np.argmax(Q[0])\n#Should be 1 or 2","execution_count":null,"outputs":[{"data":{"text/plain":"1"},"execution_count":11,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"cell_id":"00018-dc1c805a-ce9a-4e66-8dd4-bcc71d15a87f","deepnote_cell_type":"code"},"source":"np.argmax(Q[3])\n#Should be 0","execution_count":null,"outputs":[{"data":{"text/plain":"0"},"execution_count":12,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"cell_id":"00019-ded98530-c06c-4f20-b39c-f428c4d54d2d","deepnote_cell_type":"code"},"source":"np.argmax(Q[10])\n#Should be 1","execution_count":null,"outputs":[{"data":{"text/plain":"1"},"execution_count":13,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"cell_id":"00020-b2c5c017-a1cb-4bc3-b53b-981a4309c5f9","deepnote_cell_type":"code"},"source":"np.argmax(Q[14])\n#Should be 2","execution_count":null,"outputs":[{"data":{"text/plain":"2"},"execution_count":14,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","source":"## Using the Q-table","metadata":{"cell_id":"00021-5b47ddff-1933-4b16-9a26-d4ce35b3df53","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00022-40cdf250-4b00-40e5-8c4f-2b68cce3d38e","deepnote_cell_type":"code"},"source":"# Is our Q good enough to guide us from start to goal without falling into the water?\nstate = env.reset()\n\nfor step in range(10):\n    env.render()\n    # Take the action (index) with the maximum expected discounted future reward given that state\n    action = np.argmax(Q[state,:])\n    state, reward, done, info = env.step(action)","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"\n\u001b[41mS\u001b[0mFFF\nFHFH\nFFFH\nHFFG\n  (Down)\nSFFF\n\u001b[41mF\u001b[0mHFH\nFFFH\nHFFG\n  (Down)\nSFFF\nFHFH\n\u001b[41mF\u001b[0mFFH\nHFFG\n  (Right)\nSFFF\nFHFH\nF\u001b[41mF\u001b[0mFH\nHFFG\n  (Down)\nSFFF\nFHFH\nFFFH\nH\u001b[41mF\u001b[0mFG\n  (Right)\nSFFF\nFHFH\nFFFH\nHF\u001b[41mF\u001b[0mG\n  (Right)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n  (Left)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n  (Left)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n  (Left)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n"}]},{"cell_type":"code","metadata":{"cell_id":"00023-9d1c3f11-3d15-4dd0-9595-d1ebfec535a1","deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"deepnote_notebook_id":"a5a4e5c0-c14c-4419-b381-a0ac70bf6939","deepnote_execution_queue":[]}}